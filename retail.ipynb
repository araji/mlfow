{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/botocore/vendored/requests/packages/urllib3/_collections.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, MutableMapping\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit, udf\n",
    "from pyspark.sql.functions import percent_rank\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions  as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler,VectorIndexer\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipelinemlflow\n",
    "\n",
    "import mlflow\n",
    "from mlflow import spark\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "sc= SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "class UdfModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, ordered_df_columns, model_artifact):\n",
    "        self.ordered_df_columns = ordered_df_columns\n",
    "        self.model_artifact = model_artifact\n",
    "\n",
    "    def load_context(self, context):\n",
    "        import mlflow.pyfunc\n",
    "        self.spark_pyfunc = mlflow.pyfunc.load_model(context.artifacts[self.model_artifact])\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        renamed_input = model_input.rename(\n",
    "            columns={\n",
    "                str(index): column_name for index, column_name\n",
    "                    in list(enumerate(self.ordered_df_columns))\n",
    "            }\n",
    "        )\n",
    "        return self.spark_pyfunc.predict(renamed_input)\n",
    "\n",
    "def log_udf_model(artifact_path, ordered_columns, run_id):\n",
    "    udf_artifact_path = f\"udf-{artifact_path}\"\n",
    "    model_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path = udf_artifact_path,\n",
    "        python_model = UdfModelWrapper(ordered_columns, artifact_path),\n",
    "        artifacts={ artifact_path: model_uri }\n",
    "    )\n",
    "    return udf_artifact_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = sqlContext.read.parquet('hdfs://isilon.tan.lab/tpch-s1/customer.parquet')\n",
    "lineitem  = sqlContext.read.parquet('hdfs://isilon.tan.lab/tpch-s1/lineitem.parquet')\n",
    "order = sqlContext.read.parquet('hdfs://isilon.tan.lab/tpch-s1/order.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = customer.dropna()\n",
    "lineitem  = lineitem.dropna()\n",
    "order = order.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 6001215 1500000\n"
     ]
    }
   ],
   "source": [
    "print(customer.count(),lineitem.count(), order.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales = order.join(customer, order.o_custkey == customer.c_custkey, how = 'inner')\n",
    "sales = sales.join(lineitem, lineitem.l_orderkey == sales.o_orderkey, how = 'full')\n",
    "sales = sales.where('c_mktsegment == \"BUILDING\"').select('l_quantity','o_orderdate')\n",
    "\n",
    "sales = sales.groupBy('o_orderdate').agg({'l_quantity': 'sum'}) .withColumnRenamed(\"sum(l_quantity)\", \"TOTAL_SALES\") .withColumnRenamed(\"o_orderdate\", \"ORDERDATE\")\n",
    "\n",
    "\n",
    "sales = sales.withColumn('DATE', F.unix_timestamp(sales.ORDERDATE) ) \\\n",
    "            .withColumn('DAY', F.dayofmonth(sales.ORDERDATE) ) \\\n",
    "            .withColumn('WDAY', F.dayofweek(sales.ORDERDATE) )  \\\n",
    "            .withColumn('YDAY', F.dayofyear(sales.ORDERDATE) )  \\\n",
    "            .withColumn('WEEK', F.weekofyear(sales.ORDERDATE) )\n",
    "\n",
    "sales = sales.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"DATE\")))\n",
    "training = sales.where(\"rank <= .8\").drop(\"rank\").drop(\"ORDERDATE\")\n",
    "testing  = sales.where(\"rank > .8\").drop(\"rank\").drop(\"ORDERDATE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCols = training.columns\n",
    "featuresCols.remove('TOTAL_SALES')\n",
    "va = VectorAssembler(inputCols = featuresCols, outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'git-gbt' does not exist. Creating a new experiment\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']= 'AKIAIOSFODNN7EXAMPLE'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL']='http://minio.tan.lab'\n",
    "\n",
    "remote_server_uri = \"http://mlflow.tan.lab\"\n",
    "mlflow.set_tracking_uri(remote_server_uri)\n",
    "mlflow.set_experiment(\"git-gbt\")\n",
    "\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # https://github.com/mlflow/mlflow/issues/2141\n",
    "    classifier = GBTRegressor(featuresCol=\"features\", labelCol=\"TOTAL_SALES\", predictionCol=\"prediction\")\n",
    "    pipeline = Pipeline(stages=[va, classifier])\n",
    "    model = pipeline.fit(training)\n",
    "    mlflow.set_tag(\"change\",\"remove debug\")\n",
    "    mlflow.spark.log_model(model, \"gbt-model\")\n",
    "    runid = run.info.run_id\n",
    "    experiment_id = run.info.experiment_id\n",
    "    log_udf_model(\"gbt-model\",  ['DATE', 'DAY', 'WDAY', 'YDAY', 'WEEK'], runid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
